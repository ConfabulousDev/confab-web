# Session Analytics Metrics Survey

A high-level survey of Claude Code metrics, analytics, and best practices from the industry. This document captures research to inform the expansion of the Summary tab analytics in Confab.

**Date:** December 2024
**Goal:** 2-3x expansion of session analytics metrics

---

## Table of Contents

1. [Sources Reviewed](#sources-reviewed)
2. [Current State](#current-state)
3. [Available Raw Data](#available-raw-data)
4. [Industry Metrics Categories](#industry-metrics-categories)
5. [Visualizations Used in Industry](#visualizations-used-in-industry)
6. [ROI & Value Indicators](#roi--value-indicators)
7. [Gaps & Opportunities](#gaps--opportunities)
8. [Preliminary Prioritization](#preliminary-prioritization)

---

## Sources Reviewed

### Official Documentation

| Source | URL |
|--------|-----|
| Anthropic Best Practices | https://www.anthropic.com/engineering/claude-code-best-practices |
| Claude Code Monitoring Docs | https://code.claude.com/docs/en/monitoring-usage |
| Claude Code Usage Analytics | https://support.claude.com/en/articles/12157520-claude-code-usage-analytics |

### Open Source Tools

| Tool | URL | Focus |
|------|-----|-------|
| ccusage | https://github.com/ryoppippi/ccusage | CLI token analysis from JSONL |
| claude-code-otel | https://github.com/ColeMurray/claude-code-otel | Full observability stack (Grafana/Prometheus) |
| Claude-Code-Usage-Monitor | https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor | Real-time terminal dashboard |
| SigNoz Dashboard | https://signoz.io/docs/dashboards/dashboard-templates/claude-code-dashboard/ | Pre-built dashboard template |

### Industry Perspectives

| Source | URL | Focus |
|--------|-----|-------|
| Tribe AI | https://www.tribe.ai/applied-ai/a-quickstart-for-measuring-the-return-on-your-claude-code-investment | ROI measurement framework |
| Faros AI | https://www.faros.ai/blog/claude-code-token-limits | Engineering leader metrics |
| Shipyard | https://shipyard.build/blog/claude-code-track-usage/ | Usage tracking guide |
| eesel AI | https://www.eesel.ai/blog/claude-code-best-practices | Best practices compilation |
| Tembo | https://www.tembo.io/blog/mastering-claude-code-tips | Productivity tips |

### Community Resources

| Source | URL |
|--------|-----|
| ClaudeLog | https://claudelog.com/ |
| ClaudeCode.io Cost Optimization | https://claudecode.io/cost-optimization |
| Steve Kinney's Cost Management | https://stevekinney.com/courses/ai-development/cost-management |

---

## Current State

The Summary tab in Confab currently displays **3 metric cards**:

### 1. Tokens Card
- Input tokens (total sent to model)
- Output tokens (total generated by model)
- Cache created tokens
- Cache read tokens

### 2. Cost Card
- Estimated USD cost
- Uses dynamic per-model pricing (18+ models supported)
- Assumes 5-minute prompt caching

### 3. Compaction Card
- Auto-triggered compactions
- Manual compactions
- Average time for auto-compactions

### Additional
- GitHub Links Card (PRs, commits linked to session)
- Real-time polling with smart polling states
- Last updated timestamp

---

## Available Raw Data

The transcript contains rich data that is **not currently surfaced**:

### From Transcript Messages

| Data Type | Available Fields |
|-----------|------------------|
| **Timestamps** | ISO timestamp on every message |
| **Tool Calls** | Tool name, inputs, success/error status |
| **Thinking Blocks** | Level (high/med/low/off), content, signature |
| **Model Info** | Model ID, stop_reason per response |
| **File History** | Tracked files, modifications, backup versions |
| **Message Content** | Text length, content block types |
| **Todos** | Task items from TodoWrite tool |
| **Git Context** | Branch, repo URL, commit info |

### From Session Metadata

| Data Type | Available Fields |
|-----------|------------------|
| **Session Info** | ID, first_seen, last_sync_at, duration |
| **Environment** | cwd, hostname, username |
| **Files** | Transcript file info, line counts |
| **GitHub Links** | Associated PRs and commits |

---

## Industry Metrics Categories

### 1. Token & Cost Metrics

| Metric | Status | Industry Tools |
|--------|--------|----------------|
| Input/Output tokens | **Have** | All |
| Cache read/write | **Have** | All |
| Estimated cost USD | **Have** | All |
| Cost per turn | Missing | ccusage, OTEL |
| Cost per commit/PR | Missing | Tribe AI, Faros |
| 5-hour block tracking | Missing | ccusage |
| Token burn rate | Missing | Usage-Monitor |

**Industry insight:** ccusage tracks "5-hour billing windows" which align with Claude's subscription limits. Users want to know how much of their quota they've consumed.

### 2. Session & Activity Metrics

| Metric | Status | Industry Tools |
|--------|--------|----------------|
| Session duration | Missing | OTEL, SigNoz |
| Turn count | Missing | All |
| Conversation depth | Missing | SigNoz |
| Active time (excluding idle) | Missing | - |
| Activity timeline/sparkline | Missing | ccusage live |
| Burn rate (tokens/minute) | Missing | Usage-Monitor |

**Industry insight:** The Claude-Code-Usage-Monitor provides "velocity tracking and a prediction engine that estimates when current session tokens will deplete."

### 3. Tool Usage Analytics

| Metric | Status | Industry Tools |
|--------|--------|----------------|
| Tool invocation count | Missing | OTEL, SigNoz |
| Tool breakdown by type | Missing | All (pie charts) |
| Tool success rate | Missing | OTEL |
| Top tools used | Missing | SigNoz |
| Tool categories | Missing | SigNoz |
| Accept/reject decisions | Missing | SigNoz, Faros |

**Industry insight:** SigNoz dashboard includes a "Tool Types" pie chart showing Read, Edit, LS, TodoWrite, Bash distribution. This is a **high-value** visualization that appears in multiple tools.

**Tool Categories (standard groupings):**
- **File Ops:** Read, Write, Edit, Glob, NotebookEdit
- **Search:** Grep, WebSearch
- **Shell:** Bash, KillShell
- **Navigation:** Task, WebFetch
- **Planning:** TodoWrite, EnterPlanMode, ExitPlanMode
- **Communication:** AskUserQuestion

### 4. Code Productivity Metrics

| Metric | Status | Industry Tools |
|--------|--------|----------------|
| Files read | Missing | - |
| Files modified | Missing | OTEL |
| Lines of code changed | Missing | OTEL, Faros |
| Languages touched | Missing | - |
| Directories explored | Missing | - |
| Commits created | Partial (links) | OTEL, SigNoz |
| PRs created | Partial (links) | OTEL, SigNoz |

**Industry insight:** Faros notes that "Lines of code" has "low reliability" as a productivity metric. However, files touched and directory coverage can indicate session scope.

### 5. Efficiency & Quality Metrics

| Metric | Status | Industry Tools |
|--------|--------|----------------|
| Cache hit rate | Missing | Tribe AI |
| Output/input ratio | Missing | - |
| Tokens per turn | Missing | - |
| Compactions per hour | Missing | - |
| Time between compactions | Missing | - |
| Tool error rate | Missing | OTEL |

**Industry insight:** Tribe AI emphasizes "cache efficiency: comparing cache reads to cache creation operations" as a key optimization indicator.

### 6. Model & Performance Metrics

| Metric | Status | Industry Tools |
|--------|--------|----------------|
| Model distribution | Missing | SigNoz, ccusage |
| Stop reason breakdown | Missing | - |
| P95 response latency | Missing | SigNoz |
| Thinking level usage | Missing | - |
| Command duration | Missing | SigNoz |

**Industry insight:** SigNoz tracks "Command Duration (P95)" to "catch slowdowns, spikes, or performance regressions."

### 7. Task Progress Metrics

| Metric | Status | Industry Tools |
|--------|--------|----------------|
| Tasks created (TodoWrite) | Missing | - |
| Tasks completed | Missing | - |
| Task completion rate | Missing | - |
| Final todo state | Missing | - |

**Industry insight:** This appears to be a gap in existing tools. TodoWrite data is available in transcripts but not surfaced anywhere.

---

## Visualizations Used in Industry

| Visualization | Used By | Purpose |
|---------------|---------|---------|
| **Pie chart - Tool types** | SigNoz, OTEL | Quick breakdown of activity type |
| **Pie chart - Model distribution** | SigNoz, ccusage | Which models used |
| **Pie chart - Success rate** | SigNoz | Request reliability |
| **Bar chart - Top tools** | OTEL | Frequency ranking |
| **Line chart - Tokens over time** | SigNoz, ccusage | Usage trends |
| **Line chart - Cost over time** | OTEL | Spending trends |
| **Sparkline - Activity** | ccusage live | Session activity pattern |
| **Progress bar - Quota usage** | SigNoz, Usage-Monitor | Limit awareness |
| **Progress bar - Cache hit rate** | - | Efficiency indicator |
| **Table - Per-user breakdown** | SigNoz | Drill-down detail |
| **Gauge - Success rate** | OTEL | Quality indicator |
| **Heatmap - Terminal types** | SigNoz | Environment breakdown |

### Visualization Libraries to Consider

- **Recharts** - React charting library, good for pie/bar/line charts
- **Visx** - Low-level D3-based React primitives
- **Nivo** - Rich chart components with good defaults
- **Tremor** - Dashboard-focused React components

---

## ROI & Value Indicators

From Tribe AI and Faros research, these metrics correlate with productive sessions:

### Leading Indicators (Predictive)

| Indicator | What It Measures |
|-----------|------------------|
| Session → PR conversion | Sessions resulting in commits/PRs |
| Cache efficiency | High cache read = optimized workflow |
| Tool acceptance rate | Using vs. rejecting Claude suggestions |
| Low compaction frequency | Focused session vs. exploration |
| Tokens per output | Efficiency of context usage |

### Lagging Indicators (Outcome-Based)

| Indicator | What It Measures |
|-----------|------------------|
| Cost per commit | Economic efficiency |
| Cost per PR | Value delivered per dollar |
| Task completion rate | TodoWrite items completed |
| Files modified per session | Scope of changes |

### Critical Insight from Faros

> "More code doesn't mean more value. Teams saw 98% more PRs but 91% longer review times—velocity gains masked downstream bottlenecks."

This suggests we should focus on **efficiency metrics** rather than pure volume metrics.

---

## Gaps & Opportunities

### What Existing Tools Don't Cover Well

| Gap | Opportunity |
|-----|-------------|
| **Workflow pattern analysis** | What sequence of tools leads to success? |
| **File change attribution** | Which turns modified which files? |
| **Task completion tracking** | TodoWrite progress visualization |
| **Session comparison** | This session vs. your average |
| **Error recovery patterns** | How many retries to succeed? |
| **Thinking investment** | Thinking tokens vs. outcome quality |
| **Codebase coverage** | % of repo touched in session |
| **Language breakdown** | Which languages worked with |

### Differentiation Opportunities for Confab

1. **TodoWrite Visualization** - No tool currently surfaces this
2. **File-Level Attribution** - Track which files changed when
3. **Session Benchmarking** - Compare to personal/team averages
4. **Workflow Patterns** - Visualize tool sequences
5. **Language/Directory Heatmaps** - Codebase activity visualization

---

## Preliminary Prioritization

### Tier 1: High Impact, Easy to Compute

These can be computed from existing transcript data with minimal effort:

| Metric/Feature | Value | Effort |
|----------------|-------|--------|
| Session Overview Card (duration, turns, active time) | High | Low |
| Tool Usage Card (pie chart, top 5, success rate) | High | Low |
| Efficiency Card (cache hit rate, tokens/turn) | High | Low |
| Activity Timeline (sparkline) | Medium | Low |

### Tier 2: Medium Impact, Moderate Effort

| Metric/Feature | Value | Effort |
|----------------|-------|--------|
| Code Activity Card (files read/modified, languages) | High | Medium |
| Task Progress Card (TodoWrite completion) | Medium | Low |
| Token Timeline (cumulative chart) | Medium | Medium |
| Model Distribution (if multi-model) | Low | Low |

### Tier 3: Differentiation / Future

| Metric/Feature | Value | Effort |
|----------------|-------|--------|
| Session Comparison (vs. average) | High | High |
| Workflow Patterns (tool sequences) | Medium | High |
| Error Analysis (recovery time) | Medium | Medium |
| Codebase Heatmap | Medium | High |

---

## Best Practices from Anthropic

Key workflow patterns that metrics should support:

### 1. Explore-Plan-Code-Commit Pattern
> Ask Claude to read files first, then plan with extended thinking, then implement.

**Metric implication:** Track read-heavy vs. edit-heavy phases in session.

### 2. Test-Driven Development
> Write tests first, confirm failures, then implement.

**Metric implication:** Track test file modifications early in session.

### 3. Context Management
> Use `/compact` and `/clear` strategically.

**Metric implication:** Compaction frequency and timing matter.

### 4. Specificity Principle
> Explicit instructions lead to better alignment.

**Metric implication:** Shorter sessions with fewer retries = good prompts.

---

## Cost Context

Industry benchmarks for reference:

| Usage Level | Typical Cost | Notes |
|-------------|--------------|-------|
| Average developer | $6/day | Anthropic data |
| 90th percentile | <$12/day | Most users |
| Small projects (<10K LOC) | $20-100/month | Low context consumption |
| Medium projects (10K-100K LOC) | $100-200/month | Regular context management needed |
| Large projects (100K+ LOC) | $200+/month | Heavy context pressure |

---

## Next Steps

1. **Technical exploration:** Review transcript data structure in detail
2. **Prioritization:** Finalize Tier 1 metrics based on implementation effort
3. **Design:** Create UI mockups for new cards/visualizations
4. **Implementation plan:** Break down into tickets

---

## Appendix: Tool Feature Comparison

| Feature | ccusage | OTEL | SigNoz | Usage-Monitor |
|---------|---------|------|--------|---------------|
| Token tracking | Yes | Yes | Yes | Yes |
| Cost calculation | Yes | Yes | Yes | Yes |
| Model breakdown | Yes | Yes | Yes | No |
| Tool usage | No | Yes | Yes | No |
| Live monitoring | Yes | Yes | Yes | Yes |
| Session grouping | Yes | Yes | Yes | Yes |
| Quota tracking | Yes | No | Yes | Yes |
| Predictions | No | No | No | Yes |
| Self-hosted | N/A | Yes | Yes | N/A |
| Visualizations | Tables | Grafana | Dashboards | Terminal |
